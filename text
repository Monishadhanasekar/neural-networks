ðŸ”µ 1ï¸âƒ£ relu(z)
def relu(z):
    return np.maximum(0, z)

ðŸ“Œ What it does

ReLU = Rectified Linear Unit

ReLU(z)=max(0,z)

Meaning:

z value	    Output
Positive	stays same
Negative	becomes 0
0	        stays 0

ðŸ”¢ Example
z = np.array([-3, -1, 0, 2, 5])
relu(z)


Output:

[0, 0, 0, 2, 5]


It â€œturns offâ€ negative neurons.

ðŸ§  Why we use it

Without activation, the network is just:

y = Wx + b


Only straight lines â†’ no intelligence.

ReLU adds non-linearity, so network can learn complex patterns.

ðŸŸ£ 2ï¸âƒ£ relu_derivative(z)
def relu_derivative(z):
    return (z > 0).astype(float)


This is used during backpropagation.

ðŸ“Œ What it means

We need derivative of ReLU:

d/dz ReLU(z)  = {
    1  if z>0
    0  if z<=0
}

ðŸ”¢ Example
z = np.array([-3, -1, 0, 2, 5])
relu_derivative(z)


Output:

[0, 0, 0, 1, 1]

ðŸ§  Meaning in backprop

During error flow:

If neuron was active (z>0) â†’ it learns

If neuron was off (zâ‰¤0) â†’ no learning

So ReLU makes learning selective.

ðŸŽ¯ In simple words
Function	          Role
relu	        Decides which neurons fire
relu_derivative	Decides which neurons are allowed to learn

ðŸ’¡ Intuition

Think of neurons as switches:

z > 0 â†’ switch ON â†’ learning happens
z â‰¤ 0 â†’ switch OFF â†’ no learning


Thatâ€™s why ReLU is fast and powerful.